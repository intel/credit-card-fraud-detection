# Intel's Enhanced Fraud Detection Reference Kit  

This blueprint is a one click refkit to provide an end-to-end solution on how to build a credit card fraud detection model with enhanced accuracy using Graph Neural Networks (GNNs) and XGBoost. It utilizes Intel’s Enhanced Fraud Detection reference kit to capture complex behavioral patterns (e.g., fraudsters performing multiple small transactions from different cards to not get caught) and enrich the data features through Graph Neural Networks (GNN), and uses configuration-driven data-preprocessing and XGBoost-training of Classical Machine Learning workflow to efficiently build a distributed end-to-end machine learning pipeline to solve the fraud-detection task. Thus, it significantly improves the developer efficiency and boosts fraud-detection accuracy. 

## FLow
1. Click on `Use Blueprint` button.
2. You will be redirected to your blueprint flow page.
3. Go to the project settings section and update the configuration or leave as default to check on built-in demo. To change the `dir_url` in first task allows you to try with other datasources.
4. Click on the `Run Flow` button.
5. The system will automatically extract feature information from the provided dataset, train a baseline XGB model, and train a GNN-enhanced XGB model, providing an end-to-end fraud detection solution.
6. Expected output is `GNN-enhanced XGBoost model and AUCPR`. GNN-enhanced XGBoost model is an XGBoost model enhanced by GNN.  AUCPR is an evaluation metric used to measure the performance of the results, which is divided into the AUCPR of baseline XGB and the AUCPR of GNN-boosted XGB.
 
## Solution Technical Overview
In Intel's Enhanced Fraud Detection reference kit, we employ Graph Neural Networks (GNN) popular for their ability to capture complex behavioral patterns (e.g., fraudsters performing multiple small transactions from different cards to not get caught). We also demonstrate a boost in accuracy by using GNN-boosted features over a baseline trained on traditional ML-only features.  

### Highlights of Enhanced Fraud Detection Reference Use Case
- Significantly boost fraud classification accuracy by augmenting classical ML features with features generated through Graph Neural Networks (GNNs) 
- Utilize our distributed preprocessing, training and inference pipelines to detect fraud quickly
- Improve developer efficiency and experimentation with our no-code, config-driven user interface


## Solution Technical Detail
The high-level architecture of the reference use case is shown in the diagram below. We use a credit card transaction dataset open-sourced by IBM (commonly known as the tabformer dataset) in this reference use case to demonstrate the capabilities outlined in the Solution Technical Overview section. 
![folder-structure](https://github.com/intel/credit-card-fraud-detection/blob/main/assets/architecture.png)
### Task 1: Feature Engineering (Edge Featurization)
The feature engineering stage ingests the raw data, encodes each column into features using the logic defined in the feature engineering config yaml file and saves processed data. 
### Task 2: GNN Training (Node Featurization)
The GNN training stage creates homogenous graphs by consuming the processed data generated by Task 1 and trains a GraphSage model in a self-supervised link prediction task setting to learn the latent representations of the nodes (cards and merchants).  Once the GNN model is trained, the GNN workflow will concatenate the card and merchant features generated by the model to the corresponding transaction features and saves the GNN-boosted features to a CSV file.
### Task 3: XGBoost Training (Fraud Classification)
The XGBoost training stage trains a binary classification model using the data splitting, model parameters and runtime parameters set in the XGB training config yaml file. AUCPR (Area Under the Precision-Recall Curve) is used as the evaluation metric due to its robustness in evaluating highly imbalanced datasets. Data splitting is based on temporal sequence to simulate real-life scenario. The model performance on the tabformer dataset can be found in the table in [results section](#1-results). 


## Performance
### Expected Result
You should expect to see a boost in AUCPR for test split by using GNN-boosted features.

| Data split                     | Number of examples   | AUCPR - Baseline XGB  | AUCPR - GNN Boosted XGB   |
| :----------------------------: | :-----------------------: | :-------------------------: | :----------------------------: |
|   Train (year < 2018)          |     20,604,847            |            0.92             |           0.98                 |
|    Val (year = 2018)           |      1,689,822            |            0.91             |           0.93                 |
|   Test (year > 2018)           |      1,904,167            |            0.88             |           0.94                 |


### Expected output
```
Failed to read data preprocessing steps. This is either due to wrong parameters defined in the config file as shown: 'data_preprocess' or there is no need for data preprocessing.
no need for HPO
Failed to read end2end training configurations. This is either due to wrong parameters defined in the config file as shown: 'end2end_training' or there is no need for End-to-End training.
enter single-node mode...
reading training data...
reading without dropping columns...
data has the shape (24198836, 154)
start training models soon...
(24198836, 150)
read and prepare data for training...
start xgboost model training...
[0]     train-aucpr:0.48145     eval-aucpr:0.23717      test-aucpr:0.21672
[100]   train-aucpr:0.80701     eval-aucpr:0.84678      test-aucpr:0.83616
[200]   train-aucpr:0.85851     eval-aucpr:0.93348      test-aucpr:0.95080
[300]   train-aucpr:0.88907     eval-aucpr:0.93487      test-aucpr:0.95312
[400]   train-aucpr:0.91095     eval-aucpr:0.93121      test-aucpr:0.95203
[500]   train-aucpr:0.92976     eval-aucpr:0.93077      test-aucpr:0.95142
[600]   train-aucpr:0.94440     eval-aucpr:0.92910      test-aucpr:0.94902
[700]   train-aucpr:0.95611     eval-aucpr:0.92940      test-aucpr:0.94795
[800]   train-aucpr:0.96405     eval-aucpr:0.92913      test-aucpr:0.94606
[900]   train-aucpr:0.97143     eval-aucpr:0.92884      test-aucpr:0.94424
[999]   train-aucpr:0.97721     eval-aucpr:0.92824      test-aucpr:0.94275
start xgboost model testing...
testing results: aucpr on test set is 0.9427465838947949
xgboost model is saved under /workspace/tmp/models.
```

## Learn More
To read about other use cases and workflows examples, see these resources:

- [Enhanced Fraud Detection Using Graph Neural Networks with Intel Optimizations](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Enhanced-Fraud-Detection-Using-Graph-Neural-Networks-with-Intel/post/1524316)
- [Intel's Open Domain Question Answering workflow](https://github.com/intel/open-domain-question-and-answer)
- [Developer Catalog](https://developer.intel.com/aireferenceimplementations)
- [Intel® AI Analytics Toolkit (AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html)

## Support
The Credit Card Fraud Detection Github Repo tracks both bugs and enhancement requests using issues.
